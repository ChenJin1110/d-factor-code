{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b64f505-ad16-437a-94de-2646f35ae55f",
   "metadata": {
    "id": "4b64f505-ad16-437a-94de-2646f35ae55f"
   },
   "source": [
    "## Estimating lifespan normative models\n",
    "\n",
    "This notebook provides a complete walkthrough for an analysis of normative modelling using your own dataset. The tutorial is based on the teaching material at: https://github.com/CharFraza/CPC_ML_tutorial/. Here you can also find more normative modelling tutorials.\n",
    "Training and testing data is provided for this tutorial. However, the idea is that you could subsitute our provided training and testing datasets for you own dataset (as long as it matches the same format!)\n",
    "\n",
    "First, if necessary, we install PCNtoolkit (note: this tutorial requires at least version 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ec2ca6-c0a2-4abf-8f05-29edc9e0fa24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "84ec2ca6-c0a2-4abf-8f05-29edc9e0fa24",
    "outputId": "4fba3556-c6d0-4ff8-e18f-cfe2deedca3d"
   },
   "outputs": [],
   "source": [
    "# Make sure to click the restart runtime button at the\n",
    "# bottom of this code blocks' output (after you run the cell)\n",
    "! pip install pcntoolkit==0.28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c3b45-ad46-4e6d-8732-dc5ac68488c6",
   "metadata": {
    "id": "909c3b45-ad46-4e6d-8732-dc5ac68488c6"
   },
   "source": [
    "Then we import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "DGQhP2LbElmI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGQhP2LbElmI",
    "outputId": "ed447bc1-c5e4-4dfc-febb-9dd97bef5a2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'CPC_ML_tutorial'...\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/CharFraza/CPC_ML_tutorial.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d451c106-08e2-4f5b-baf9-da240768e68b",
   "metadata": {
    "id": "d451c106-08e2-4f5b-baf9-da240768e68b"
   },
   "outputs": [],
   "source": [
    "# we need to be in the CPC_ML_tutorial folder when we import the libraries in the code block below,\n",
    "# because there is a function called nm_utils that is in this folder that we need to import\n",
    "import os\n",
    "os.chdir('CPC_ML_tutorial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83c494d3-6ebd-4cde-aff0-8fc9344374dd",
   "metadata": {
    "id": "83c494d3-6ebd-4cde-aff0-8fc9344374dd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pcntoolkit.normative import estimate, predict, evaluate\n",
    "from pcntoolkit.util.utils import compute_MSLL, create_design_matrix\n",
    "from nm_utils import calibration_descriptives, remove_bad_subjects, load_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9822cc19-48e9-428b-8c5e-e059fd2d23f7",
   "metadata": {
    "id": "9822cc19-48e9-428b-8c5e-e059fd2d23f7"
   },
   "source": [
    "Now, we configure the locations in which the data are stored.\n",
    "\n",
    "**Notes:**\n",
    "- The data are assumed to be in CSV format and will be loaded as pandas dataframes\n",
    "- Generally the raw data will be in a different location to the analysis\n",
    "- The data can have arbitrary columns but some are required by the script, i.e. 'age', 'sex' and 'site', plus the phenotypes you wish to estimate (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da01c88-7033-498b-a811-79ad58e8c17a",
   "metadata": {
    "id": "7da01c88-7033-498b-a811-79ad58e8c17a"
   },
   "outputs": [],
   "source": [
    "# where the raw data are stored\n",
    "data_dir = 'data'\n",
    "\n",
    "# where the analysis takes place\n",
    "root_dir = 'D:/python_code/NM_educational_OHBM24-main/NM_educational_OHBM24-main/slot1_Fraza/gmv_test'\n",
    "out_dir = os.path.join(root_dir,'models','test')\n",
    "\n",
    "# create the output directory if it does not already exist\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01141f19-a960-4823-baad-8604975304c3",
   "metadata": {
    "id": "01141f19-a960-4823-baad-8604975304c3"
   },
   "source": [
    "Now we load the data.\n",
    "\n",
    "We will load one pandas dataframe for the training set and one dataframe for the test set. We also configure a list of site ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e22292c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"work1 code\"\"\"\n",
    "train = pd.read_csv(r'D:\\python_code\\NM_educational_OHBM24-main\\NM_educational_OHBM24-main\\slot1_Fraza\\data_UKBB1\\GMV_health_ins2.csv')\n",
    "test = pd.read_csv(r'D:\\python_code\\NM_educational_OHBM24-main\\NM_educational_OHBM24-main\\slot1_Fraza\\data_UKBB1\\GMV_disease_ins2.csv')\n",
    "Site = pd.read_csv(r'D:\\python_code\\NM_educational_OHBM24-main\\NM_educational_OHBM24-main\\slot1_Fraza\\data_UKBB1\\Site_chu.csv')\n",
    "Site2 = Site[['eid', 'site']]\n",
    "label = pd.read_csv(r'D:\\python_code\\NM_educational_OHBM24-main\\NM_educational_OHBM24-main\\slot1_Fraza\\data_UKBB1\\GMV_labels.csv')\n",
    "train.columns = train.columns[:1].tolist() + label['Description'].to_list() + train.columns[-2:].tolist()\n",
    "test.columns = test.columns[:1].tolist() + label['Description'].to_list() + test.columns[-2:].tolist()\n",
    "train = pd.merge(train, Site2, on='eid')\n",
    "test = pd.merge(test, Site2, on='eid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8311e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(r'D:\\python_code\\NM_educational_OHBM24-main\\NM_educational_OHBM24-main\\slot1_Fraza\\data_UKBB1\\train_data.csv')\n",
    "test.to_csv(r'D:\\python_code\\NM_educational_OHBM24-main\\NM_educational_OHBM24-main\\slot1_Fraza\\data_UKBB1\\test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "850fee6b-421f-41d9-8fd6-7e1dafbf0e9f",
   "metadata": {
    "id": "850fee6b-421f-41d9-8fd6-7e1dafbf0e9f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_tr = pd.read_csv(r'D:\\python_code\\NM_educational_OHBM24-main\\NM_educational_OHBM24-main\\slot1_Fraza\\data_UKBB1_gmv\\train_data.csv', index_col=0)\n",
    "df_te = pd.read_csv(r'D:\\python_code\\NM_educational_OHBM24-main\\NM_educational_OHBM24-main\\slot1_Fraza\\data_UKBB1_gmv\\test_data.csv', index_col=0)\n",
    "\n",
    "# extract a list of unique site ids from the training set\n",
    "site_ids = sorted(set(df_tr['site'].to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f9593a-d3c9-4d08-a877-8794203c0001",
   "metadata": {
    "id": "29f9593a-d3c9-4d08-a877-8794203c0001"
   },
   "source": [
    "### Configure which models to fit\n",
    "\n",
    "Next, we load the image derived phenotypes (IDPs) which we will process in this analysis. This is effectively just a list of columns in your dataframe. Here we estimate normative models for the left hemisphere, right hemisphere and cortical structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7438ef7e-9340-4f13-8d57-816918923705",
   "metadata": {
    "id": "7438ef7e-9340-4f13-8d57-816918923705"
   },
   "outputs": [],
   "source": [
    "# we choose here to process all idps. Uncomment lines 2-7 (and comment line 11) to run models for the whole brain, but we suggest just starting with several ROIs\n",
    "#os.chdir(root_dir)\n",
    "#!wget -nc https://raw.githubusercontent.com/CharFraza/CPC_ML_tutorial/master/data/task1_phenotypes.txt\n",
    "#with open(os.path.join(root_dir,'task1_phenotypes.txt')) as f:\n",
    "#  idp_ids = f.read().splitlines()\n",
    "#for idx, ele in enumerate(idp_ids):\n",
    "#        idp_ids[idx] = ele.replace('\\t', '')\n",
    "\n",
    "# we could also just specify a list of IDPs. Use this line to run just 2 models (not the whole brain)...this is a good place to start. If you have time,\n",
    "# you can uncomment the above line and run the whole brain models. Be sure to comment out this line if you uncomment the above line.\n",
    "idp_ids = df_tr.columns[4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d791db6-8fe5-450c-88eb-84a390b8753a",
   "metadata": {
    "id": "5d791db6-8fe5-450c-88eb-84a390b8753a"
   },
   "source": [
    "### Configure model parameters\n",
    "\n",
    "Now, we configure some parameters for the regression model we use to fit the normative model. Here we will use a 'warped' Bayesian linear regression model. To model non-Gaussianity, we select a sin arcsinh warp and to model non-linearity, we stick with the default value for the basis expansion (a cubic b-spline basis set with 5 knot points). Since we are sticking with the default value, we do not need to specify any parameters for this, but we do need to specify the limits. We choose to pad the input by a few years either side of the input range. We will also set a couple of options that control the estimation of the model\n",
    "\n",
    "For further details about the likelihood warping approach, see [Fraza et al 2021](https://www.biorxiv.org/content/10.1101/2021.04.05.438429v1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0IYl-eg2xGWE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0IYl-eg2xGWE",
    "outputId": "b881d93e-bdda-4c5d-e42b-59106ff59a03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    24014.000000\n",
       "mean        63.297021\n",
       "std          7.446508\n",
       "min         45.166667\n",
       "25%         57.333333\n",
       "50%         63.583333\n",
       "75%         69.083333\n",
       "max         82.250000\n",
       "Name: age, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the min & max age of the dataset, use this info to update the xmin & xmax variables in the code block below.\n",
    "df_tr['age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e44e257c-676e-49d8-89ec-657e506c3b74",
   "metadata": {
    "id": "e44e257c-676e-49d8-89ec-657e506c3b74"
   },
   "outputs": [],
   "source": [
    "# which data columns do we wish to use as covariates?\n",
    "# You could add additional covariates from your own dataset here that you wish to use as predictors.\n",
    "# However, for this tutorial today we will keep it simple and just use age & sex.\n",
    "# Maybe discuss with your partner ideas you have for other covariates you would like to include.\n",
    "cols_cov = ['age','sex']\n",
    "\n",
    "# which warping function to use? We can set this to None in order to fit a vanilla Gaussian noise model\n",
    "warp =  'WarpSinArcsinh'\n",
    "#warp = None\n",
    "\n",
    "# limits for cubic B-spline basis\n",
    "# check the min & max ages of the dataframes, add 5 to the max\n",
    "# and subtract 5 from the min and adjust these variables accordingly\n",
    "xmin = 45 # set this variable\n",
    "xmax = 83 # set this variable\n",
    "\n",
    "# Do we want to force the model to be refit every time?\n",
    "# When training normative model from scratch like we are doing in this notebook (not re-using a pre-trained model),\n",
    "# this variable should be = True\n",
    "force_refit = True\n",
    "\n",
    "# Absolute Z treshold above which a sample is considered to be an outlier (without fitting any model)\n",
    "outlier_thresh = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896842d7-8913-4137-9d86-4757c42bcf1b",
   "metadata": {
    "id": "896842d7-8913-4137-9d86-4757c42bcf1b"
   },
   "source": [
    "### Fit the models\n",
    "\n",
    "Now we fit the models. This involves looping over the IDPs we have selected. We will use a module from PCNtoolkit to set up the design matrices, containing the covariates, fixed effects for site and nonlinear basis expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e9b50c-574b-4e2c-a511-cc444db4393e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4e9b50c-574b-4e2c-a511-cc444db4393e",
    "outputId": "e9d976c1-f0ee-4317-bf0b-2e5b81140c3e"
   },
   "outputs": [],
   "source": [
    "for idp_num, idp in enumerate(idp_ids):\n",
    "    print('Running IDP', idp_num, idp, ':')\n",
    "\n",
    "    idp_dir = os.path.join(out_dir, idp)\n",
    "    os.makedirs(idp_dir, exist_ok=True)\n",
    "    os.chdir(idp_dir)\n",
    "\n",
    "    # extract the response variables for training and test set\n",
    "    y_tr = df_tr[idp].to_numpy()\n",
    "    y_te = df_te[idp].to_numpy()\n",
    "\n",
    "    # remove gross outliers and implausible values\n",
    "    yz_tr = (y_tr - np.mean(y_tr)) / np.std(y_tr)\n",
    "    yz_te = (y_te - np.mean(y_te)) / np.std(y_te)\n",
    "    nz_tr = np.bitwise_and(np.abs(yz_tr) < outlier_thresh, y_tr > 0)\n",
    "    nz_te = np.bitwise_and(np.abs(yz_te) < outlier_thresh, y_te > 0)\n",
    "    y_tr = y_tr[nz_tr]\n",
    "    y_te = y_te[nz_te]\n",
    "\n",
    "    eid = df_te['eid'][nz_te]\n",
    "    eid_file_te = os.path.join(idp_dir, 'eid.txt')\n",
    "    np.savetxt(eid_file_te, eid, fmt='%d')\n",
    "\n",
    "    # write out the response variables for training and test\n",
    "    resp_file_tr = os.path.join(idp_dir, 'resp_tr.txt')\n",
    "    resp_file_te = os.path.join(idp_dir, 'resp_te.txt')\n",
    "    np.savetxt(resp_file_tr, y_tr)\n",
    "    np.savetxt(resp_file_te, y_te)\n",
    "\n",
    "    # configure the design matrix\n",
    "    X_tr = create_design_matrix(df_tr[cols_cov].loc[nz_tr],\n",
    "                                site_ids = df_tr['site'].loc[nz_tr],\n",
    "                                basis = 'bspline',\n",
    "                                p = 3,\n",
    "                                nknots = 4,\n",
    "                                xmin = xmin,\n",
    "                                xmax = xmax)\n",
    "    X_te = create_design_matrix(df_te[cols_cov].loc[nz_te],\n",
    "                                site_ids = df_te['site'].loc[nz_te],\n",
    "                                basis = 'bspline',\n",
    "                                p = 3,\n",
    "                                nknots = 4,\n",
    "                                xmin = xmin,\n",
    "                                xmax = xmax)\n",
    "\n",
    "    # configure and save the covariates\n",
    "    cov_file_tr = os.path.join(idp_dir, 'cov_bspline_tr.txt')\n",
    "    cov_file_te = os.path.join(idp_dir, 'cov_bspline_te.txt')\n",
    "    np.savetxt(cov_file_tr, X_tr)\n",
    "    np.savetxt(cov_file_te, X_te)\n",
    "\n",
    "    if not force_refit and os.path.exists(os.path.join(idp_dir, 'Models', 'NM_0_0_estimate.pkl')):\n",
    "        print('Making predictions using a pre-existing model...')\n",
    "        suffix = 'predict'\n",
    "\n",
    "        # Make prdictsion with test data\n",
    "        predict(cov_file_te,\n",
    "                alg='blr',\n",
    "                respfile=resp_file_te,\n",
    "                model_path=os.path.join(idp_dir,'Models'),\n",
    "                outputsuffix=suffix)\n",
    "    else:\n",
    "        print('Estimating the normative model...')\n",
    "        estimate(cov_file_tr, resp_file_tr, testresp=resp_file_te,\n",
    "                 testcov=cov_file_te, alg='blr', optimizer = 'powell', cvfolds=10,\n",
    "                 savemodel=True, warp=warp, warp_reparam=True) \n",
    "        suffix = 'estimate'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w1Qo37JIa4-x",
   "metadata": {
    "id": "w1Qo37JIa4-x"
   },
   "source": [
    "## Questions\n",
    "1. Can you explain the covariance matrix and it's elements?\n",
    "2. How do you change the order of the spline function?\n",
    "3. How do you change the number of knots?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5Wz9XrRgaw2T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Wz9XrRgaw2T",
    "outputId": "825ca0d6-95b3-4259-9eb0-44ba18534cd1"
   },
   "outputs": [],
   "source": [
    "print(pd.DataFrame(X_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f77cf-c873-4047-91ac-50b9571704fd",
   "metadata": {
    "id": "925f77cf-c873-4047-91ac-50b9571704fd"
   },
   "source": [
    "### Compute error metrics\n",
    "\n",
    "In this section we compute the following error metrics for all IDPs (all evaluated on the test set): assess the goodness of fit between the predicted probabilities of a model and the actual observed outcomes.\n",
    "- Negative log likelihood (NLL): NLL assesses the goodness of fit between the predicted probabilities of a model and the actual observed outcomes. In this case, it measures the discrepancy between the predicted probabilities of the model for the IDPs (Independent Data Points) and the actual outcomes on the test set.\n",
    "- Explained variance (EV): EV assesses how much of the total variation in the dependent variable (IDP) is explained by the independent variables. In the context of this analysis, it quantifies the extent to which the independent variables account for the variability observed in the IDPs on the test set.\n",
    "- Mean standardized log loss (MSLL): MSLL takes into account both the mean error and the estimated prediction variance. It is used to evaluate the performance of the model, and in this case, a lower MSLL indicates a better-fitting model for the IDPs on the test set.\n",
    "- Bayesian information criteria (BIC): BIC is a model selection criterion that balances the goodness of fit to the data with the model's complexity. It penalizes models with higher flexibility and aims to find the best trade-off. Lower BIC scores indicate models that better explain the IDPs on the test set while considering the model complexity.\n",
    "- Skew and Kurtosis of the Z-distribution: Skewness and kurtosis are statistical measures used to assess the shape and characteristics of a distribution. They provide information about how well the warping function performed in terms of capturing the departure from a normal distribution for the IDPs.\n",
    "\n",
    "For more information on the different model selection criteria see [Fraza et al 2021](https://www.biorxiv.org/content/10.1101/2021.04.05.438429v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9d7500-4f46-4ee1-9756-81758ae5b1d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "2e9d7500-4f46-4ee1-9756-81758ae5b1d1",
    "outputId": "b87bd818-bdf5-4cf3-83ac-4272c742bfd0"
   },
   "outputs": [],
   "source": [
    "# initialise dataframe we will use to store quantitative metrics\n",
    "blr_metrics = pd.DataFrame(columns = ['eid', 'NLL', 'EV', 'MSLL', 'BIC','Skew','Kurtosis'])\n",
    "\n",
    "for idp_num, idp in enumerate(idp_ids):\n",
    "    print('Running IDP', idp_num, idp, ':')\n",
    "\n",
    "    idp_dir = os.path.join(out_dir, idp)\n",
    "\n",
    "    # load the predictions and true data. We use a custom function that ensures 2d arrays\n",
    "    # equivalent to: y = np.loadtxt(filename); y = y[:, np.newaxis]\n",
    "    yhat_te = load_2d(os.path.join(idp_dir, 'yhat_' + suffix + '.txt'))\n",
    "    s2_te = load_2d(os.path.join(idp_dir, 'ys2_' + suffix + '.txt'))\n",
    "    y_te = load_2d(os.path.join(idp_dir, 'resp_te.txt'))\n",
    "\n",
    "    with open(os.path.join(idp_dir,'Models', 'NM_0_0_estimate.pkl'), 'rb') as handle:\n",
    "        nm = pickle.load(handle)\n",
    "\n",
    "    # compute error metrics\n",
    "    if warp is None:\n",
    "        metrics = evaluate(y_te, yhat_te)\n",
    "\n",
    "        # compute MSLL manually as a sanity check\n",
    "        y_tr_mean = np.array( [[np.mean(y_tr)]] )\n",
    "        y_tr_var = np.array( [[np.var(y_tr)]] )\n",
    "        MSLL = compute_MSLL(y_te, yhat_te, s2_te, y_tr_mean, y_tr_var)\n",
    "    else:\n",
    "        warp_param = nm.blr.hyp[1:nm.blr.warp.get_n_params()+1]\n",
    "        W = nm.blr.warp\n",
    "\n",
    "        # warp predictions\n",
    "        med_te = W.warp_predictions(np.squeeze(yhat_te), np.squeeze(s2_te), warp_param)[0]\n",
    "        med_te = med_te[:, np.newaxis]\n",
    "\n",
    "        # evaluation metrics\n",
    "        metrics = evaluate(y_te, med_te)\n",
    "\n",
    "        # compute MSLL manually\n",
    "        y_te_w = W.f(y_te, warp_param)\n",
    "        y_tr_w = W.f(y_tr, warp_param)\n",
    "        y_tr_mean = np.array( [[np.mean(y_tr_w)]] )\n",
    "        y_tr_var = np.array( [[np.var(y_tr_w)]] )\n",
    "        MSLL = compute_MSLL(y_te_w, yhat_te, s2_te, y_tr_mean, y_tr_var)\n",
    "\n",
    "    Z = np.loadtxt(os.path.join(idp_dir, 'Z_' + suffix + '.txt'))\n",
    "    [skew, sdskew, kurtosis, sdkurtosis, semean, sesd] = calibration_descriptives(Z)\n",
    "\n",
    "    BIC = len(nm.blr.hyp) * np.log(y_tr.shape[0]) + 2 * nm.neg_log_lik\n",
    "\n",
    "    blr_metrics.loc[len(blr_metrics)] = [idp, nm.neg_log_lik, metrics['EXPV'][0],\n",
    "                                         MSLL[0], BIC, skew, kurtosis]\n",
    "\n",
    "display(blr_metrics)\n",
    "\n",
    "blr_metrics.to_csv(os.path.join(out_dir,'blr_metrics.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "znBtH9lxA_wJ",
   "metadata": {
    "id": "znBtH9lxA_wJ"
   },
   "source": [
    "## Questions to discuss\n",
    "1. Model selection: Which model selection criteria would you use to choose the optimal model?\n",
    "2. Model flexibility: What happens when you change the warping or B-spline settings?\n",
    "3. Bias-variance tradeoff: How would you consider the bias-variance tradeoff when deciding the models parameters?\n",
    "4. Which independent variables do you think are important to add to the normative model?\n",
    "5. Are there other model selection criteria that you think should be considered?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4WsaKT7OehjV",
   "metadata": {
    "id": "4WsaKT7OehjV"
   },
   "source": [
    "## Suggested further readings\n",
    "\n",
    "1. [PCNtoolkit Background](https://pcntoolkit.readthedocs.io/en/latest/pages/pcntoolkit_background.html)\n",
    "2. [Conceptualizing mental disorders as deviations from normative functioning](https://www.nature.com/articles/s41380-019-0441-1)\n",
    "3. [Understanding Heterogeneity in Clinical Cohorts Using Normative Models: Beyond Case-Control Studies\n",
    "](https://www.sciencedirect.com/science/article/pii/S0006322316000020)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db73f61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configuring dummy data ...\n"
     ]
    }
   ],
   "source": [
    "# which sex do we want to plot?\n",
    "sex = 1 # 1 = male 0 = female\n",
    "if sex == 1:\n",
    "    clr = 'blue';\n",
    "else:\n",
    "    clr = 'red'\n",
    "\n",
    "# create dummy data for visualisation\n",
    "print('configuring dummy data ...')\n",
    "xx = np.arange(xmin, xmax, 0.5)\n",
    "X0_dummy = np.zeros((len(xx), 2))\n",
    "X0_dummy[:,0] = xx\n",
    "X0_dummy[:,1] = sex\n",
    "\n",
    "# create the design matrix\n",
    "X_dummy = create_design_matrix(X0_dummy, xmin=xmin, xmax=xmax, site_ids=None ,p=3 ,nknots=4 ,all_sites=site_ids)\n",
    "\n",
    "# save the dummy covariates\n",
    "cov_file_dummy = os.path.join(out_dir,'cov_bspline_dummy_mean.txt')\n",
    "np.savetxt(cov_file_dummy, X_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1263f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white')\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "\n",
    "for idp_num, idp in enumerate(idp_ids):\n",
    "    #idp_short = extract_relevant_part(idp)\n",
    "    print('Running IDP', idp_num, idp, ':')\n",
    "\n",
    "    idp_dir = os.path.join(out_dir, idp)\n",
    "    os.chdir(idp_dir)\n",
    "\n",
    "    # load the true data points\n",
    "    yhat_te = load_2d(os.path.join(idp_dir, 'yhat_estimate.txt'))\n",
    "    s2_te = load_2d(os.path.join(idp_dir, 'ys2_estimate.txt'))\n",
    "    y_te = load_2d(os.path.join(idp_dir, 'resp_te.txt'))\n",
    "\n",
    "    plt.figure(figsize=(3, 3))\n",
    "\n",
    "    # set up the covariates for the dummy data\n",
    "    print('Making predictions with dummy covariates (for visualisation)')\n",
    "    yhat, s2 = predict(cov_file_dummy,\n",
    "                       alg = 'blr',\n",
    "                       respfile = None,\n",
    "                       model_path = os.path.join(idp_dir,'Models'),\n",
    "                       outputsuffix = '_dummy')\n",
    "\n",
    "    # load the normative model\n",
    "    with open(os.path.join(idp_dir,'Models', 'NM_0_0_estimate.pkl'), 'rb') as handle:\n",
    "        nm = pickle.load(handle)\n",
    "\n",
    "    # get the warp and warp parameters\n",
    "    W = nm.blr.warp\n",
    "    warp_param = nm.blr.hyp[1:nm.blr.warp.get_n_params()+1]\n",
    "\n",
    "    # first, we warp predictions for the true data and compute evaluation metrics\n",
    "    med_te = W.warp_predictions(np.squeeze(yhat_te), np.squeeze(s2_te), warp_param)[0]\n",
    "    med_te = med_te[:, np.newaxis]\n",
    "    print('metrics:', evaluate(y_te, med_te))\n",
    "\n",
    "    # then, we warp dummy predictions to create the plots\n",
    "    med, pr_int = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2), warp_param)\n",
    "\n",
    "    # extract the different variance components to visualise\n",
    "    beta, junk1, junk2 = nm.blr._parse_hyps(nm.blr.hyp, X_dummy)\n",
    "    s2n = 1/beta # variation (aleatoric uncertainty)\n",
    "    s2s = s2-s2n # modelling uncertainty (epistemic uncertainty)\n",
    "\n",
    "    # plot the data points\n",
    "    y_te_rescaled_all = np.zeros_like(y_te)\n",
    "    for sid, site in enumerate(site_ids):\n",
    "        # plot the true test data points\n",
    "        if all(elem in site_ids for elem in site_ids):\n",
    "            # all data in the test set are present in the training set\n",
    "\n",
    "            # first, we select the data points belonging to this particular site\n",
    "            idx = np.where(np.bitwise_and(X_te[:,2] == sex, X_te[:, sid + len(cols_cov) + 1] != 0))[0]\n",
    "            if len(idx) == 0:\n",
    "                print('No data for site', sid, site, 'skipping...')\n",
    "                continue\n",
    "            idx = idx[idx < len(y_te)]\n",
    "            # then directly adjust the data\n",
    "            idx_dummy = np.bitwise_and(X_dummy[:,1] > X_te[idx,1].min(), X_dummy[:,1] < X_te[idx,1].max())\n",
    "            y_te_rescaled = y_te[idx] - np.median(y_te[idx]) + np.median(med[idx_dummy])\n",
    "        else:\n",
    "            # we need to adjust the data based on the adaptation dataset\n",
    "\n",
    "            # first, select the data point belonging to this particular site\n",
    "            idx = np.where(np.bitwise_and(X_te[:,2] == sex, (df_te['site'] == site).to_numpy()))[0]\n",
    "\n",
    "            # load the adaptation data\n",
    "            y_ad = load_2d(os.path.join(idp_dir, 'resp_ad.txt'))\n",
    "            X_ad = load_2d(os.path.join(idp_dir, 'cov_bspline_ad.txt'))\n",
    "            idx_a = np.where(np.bitwise_and(X_ad[:,2] == sex, (df_tr['site'] == site).to_numpy()))[0]\n",
    "            if len(idx) < 2 or len(idx_a) < 2:\n",
    "                print('Insufficent data for site', sid, site, 'skipping...')\n",
    "                continue\n",
    "\n",
    "            # adjust and rescale the data\n",
    "            y_te_rescaled, s2_rescaled = nm.blr.predict_and_adjust(nm.blr.hyp,\n",
    "                                                                   X_ad[idx_a,:],\n",
    "                                                                   np.squeeze(y_ad[idx_a]),\n",
    "                                                                   Xs=None,\n",
    "                                                                   ys=np.squeeze(y_te[idx]))\n",
    "        # plot the (adjusted) data points\n",
    "        plt.scatter(X_te[idx,1], y_te_rescaled, s=5, color=clr, alpha = 0.1)\n",
    "\n",
    "    # plot the median of the dummy data\n",
    "    plt.plot(xx, med, clr)\n",
    "\n",
    "    # fill the gaps in between the centiles\n",
    "    junk, pr_int25 = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2), warp_param, percentiles=[0.25,0.75])\n",
    "    junk, pr_int95 = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2), warp_param, percentiles=[0.05,0.95])\n",
    "    #junk, pr_int99 = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2), warp_param, percentiles=[0.01,0.99])\n",
    "    plt.fill_between(xx, pr_int25[:,0], pr_int25[:,1], alpha = 0.1,color=clr)\n",
    "    plt.fill_between(xx, pr_int95[:,0], pr_int95[:,1], alpha = 0.1,color=clr)\n",
    "    #plt.fill_between(xx, pr_int99[:,0], pr_int99[:,1], alpha = 0.1,color=clr)\n",
    "\n",
    "    # make the width of each centile proportional to the epistemic uncertainty\n",
    "    junk, pr_int25l = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2-0.5*s2s), warp_param, percentiles=[0.25,0.75])\n",
    "    junk, pr_int95l = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2-0.5*s2s), warp_param, percentiles=[0.05,0.95])\n",
    "    #junk, pr_int99l = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2-0.5*s2s), warp_param, percentiles=[0.01,0.99])\n",
    "    junk, pr_int25u = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2+0.5*s2s), warp_param, percentiles=[0.25,0.75])\n",
    "    junk, pr_int95u = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2+0.5*s2s), warp_param, percentiles=[0.05,0.95])\n",
    "    #junk, pr_int99u = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2+0.5*s2s), warp_param, percentiles=[0.01,0.99])\n",
    "    plt.fill_between(xx, pr_int25l[:,0], pr_int25u[:,0], alpha = 0.3,color=clr)\n",
    "    plt.fill_between(xx, pr_int95l[:,0], pr_int95u[:,0], alpha = 0.3,color=clr)\n",
    "    #plt.fill_between(xx, pr_int99l[:,0], pr_int99u[:,0], alpha = 0.3,color=clr)\n",
    "    plt.fill_between(xx, pr_int25l[:,1], pr_int25u[:,1], alpha = 0.3,color=clr)\n",
    "    plt.fill_between(xx, pr_int95l[:,1], pr_int95u[:,1], alpha = 0.3,color=clr)\n",
    "    #plt.fill_between(xx, pr_int99l[:,1], pr_int99u[:,1], alpha = 0.3,color=clr)\n",
    "\n",
    "    # plot actual centile lines\n",
    "    plt.plot(xx, pr_int25[:,0],color=clr, linewidth=0.5)\n",
    "    plt.plot(xx, pr_int25[:,1],color=clr, linewidth=0.5)\n",
    "    plt.plot(xx, pr_int95[:,0],color=clr, linewidth=0.5)\n",
    "    plt.plot(xx, pr_int95[:,1],color=clr, linewidth=0.5)\n",
    "    #plt.plot(xx, pr_int99[:,0],color=clr, linewidth=0.5)\n",
    "    #plt.plot(xx, pr_int99[:,1],color=clr, linewidth=0.5)\n",
    "\n",
    "    plt.xlabel('Age', fontweight='bold')\n",
    "    plt.ylabel('GMV', fontweight='bold')\n",
    "    plt.title(idp, fontweight='bold')\n",
    "    plt.xlim((45,83))\n",
    "    plt.savefig(os.path.join(idp_dir, 'centiles_' + str(sex) + '.svg'), bbox_inches='tight', transparent=True, format='svg', dpi=300)\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "\n",
    "os.chdir(out_dir)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "1_fit_normative_models.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "pcn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
